{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0. Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs=784 + 1 # bias input\n",
    "hidden=100\n",
    "outputs=10\n",
    "ticks = 3\n",
    "\n",
    "batch_size=64\n",
    "epochs = 100\n",
    "lr = 0.001"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(\n",
    "  torchvision.datasets.MNIST(\"/tmp/mnist_data\", train=True, download=True,\n",
    "                             transform=torchvision.transforms.Compose([\n",
    "                               torchvision.transforms.ToTensor(),\n",
    "                               torchvision.transforms.Normalize(\n",
    "                                 (0.1307,), (0.3081,))\n",
    "                             ])),\n",
    "  batch_size=batch_size, shuffle=True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "  torchvision.datasets.MNIST(\"/tmp/mnist_data\", train=False, download=True,\n",
    "                             transform=torchvision.transforms.Compose([\n",
    "                               torchvision.transforms.ToTensor(),\n",
    "                               torchvision.transforms.Normalize(\n",
    "                                 (0.1307,), (0.3081,))\n",
    "                             ])),\n",
    "  batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2 MNN forward definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def net(s, x):\n",
    "    s[:,0:inputs-1] = x\n",
    "    s[:,inputs] = 1\n",
    "    t = torch.matmul(s, A)\n",
    "    s = torch.relu(t)\n",
    "    return s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0  loss: 3372.194092  acc: 0.062500\n",
      "epoch: 0  loss: 26.474407  acc: 0.359375\n",
      "epoch: 0  loss: 6.621606  acc: 0.421875\n",
      "epoch: 0  loss: 8.578157  acc: 0.515625\n",
      "epoch: 0  loss: 5.717260  acc: 0.390625\n",
      "epoch: 1  loss: 27.072432  acc: 0.468750\n",
      "epoch: 1  loss: 8.286741  acc: 0.375000\n",
      "epoch: 1  loss: 2.454543  acc: 0.468750\n",
      "epoch: 1  loss: 6.169957  acc: 0.500000\n",
      "epoch: 1  loss: 1.970924  acc: 0.390625\n",
      "epoch: 2  loss: 7.954168  acc: 0.421875\n",
      "epoch: 2  loss: 1.510525  acc: 0.375000\n",
      "epoch: 2  loss: 30.964554  acc: 0.281250\n",
      "epoch: 2  loss: 6.005965  acc: 0.343750\n",
      "epoch: 2  loss: 6.202227  acc: 0.312500\n",
      "epoch: 3  loss: 25.473427  acc: 0.359375\n",
      "epoch: 3  loss: 1.553840  acc: 0.390625\n",
      "epoch: 3  loss: 3.897541  acc: 0.296875\n",
      "epoch: 3  loss: 10.986524  acc: 0.328125\n",
      "epoch: 3  loss: 1.484676  acc: 0.375000\n",
      "epoch: 4  loss: 4.381802  acc: 0.359375\n",
      "epoch: 4  loss: 12.233021  acc: 0.312500\n",
      "epoch: 4  loss: 8.849322  acc: 0.203125\n",
      "epoch: 4  loss: 1.799240  acc: 0.234375\n",
      "epoch: 4  loss: 19.747335  acc: 0.203125\n",
      "epoch: 5  loss: 7.137672  acc: 0.234375\n",
      "epoch: 5  loss: 2.619088  acc: 0.250000\n",
      "epoch: 5  loss: 1.872229  acc: 0.234375\n",
      "epoch: 5  loss: 1.994566  acc: 0.250000\n",
      "epoch: 5  loss: 3.376936  acc: 0.234375\n",
      "epoch: 6  loss: 5.223537  acc: 0.156250\n",
      "epoch: 6  loss: 2.053472  acc: 0.218750\n",
      "epoch: 6  loss: 1.986874  acc: 0.250000\n",
      "epoch: 6  loss: 2.014761  acc: 0.156250\n",
      "epoch: 6  loss: 1.834895  acc: 0.250000\n",
      "epoch: 7  loss: 2.181653  acc: 0.203125\n",
      "epoch: 7  loss: 1.944925  acc: 0.250000\n",
      "epoch: 7  loss: 2.050739  acc: 0.140625\n",
      "epoch: 7  loss: 2.145448  acc: 0.187500\n",
      "epoch: 7  loss: 2.051136  acc: 0.171875\n",
      "epoch: 8  loss: 2.009058  acc: 0.171875\n",
      "epoch: 8  loss: 2.120372  acc: 0.171875\n",
      "epoch: 8  loss: 2.034995  acc: 0.171875\n",
      "epoch: 8  loss: 2.194650  acc: 0.093750\n",
      "epoch: 8  loss: 2.384797  acc: 0.187500\n",
      "epoch: 9  loss: 1.949973  acc: 0.218750\n",
      "epoch: 9  loss: 1.947633  acc: 0.218750\n",
      "epoch: 9  loss: 2.029581  acc: 0.156250\n",
      "epoch: 9  loss: 2.202549  acc: 0.156250\n",
      "epoch: 9  loss: 2.086825  acc: 0.140625\n",
      "epoch: 10  loss: 2.027512  acc: 0.218750\n",
      "epoch: 10  loss: 2.041517  acc: 0.171875\n",
      "epoch: 10  loss: 2.015082  acc: 0.187500\n",
      "epoch: 10  loss: 1.978892  acc: 0.234375\n",
      "epoch: 10  loss: 1.952677  acc: 0.265625\n",
      "epoch: 11  loss: 1.985299  acc: 0.187500\n",
      "epoch: 11  loss: 2.077671  acc: 0.125000\n",
      "epoch: 11  loss: 2.697185  acc: 0.203125\n",
      "epoch: 11  loss: 1.985947  acc: 0.187500\n",
      "epoch: 11  loss: 2.685299  acc: 0.203125\n",
      "epoch: 12  loss: 3.820582  acc: 0.109375\n",
      "epoch: 12  loss: 2.082453  acc: 0.125000\n",
      "epoch: 12  loss: 2.054185  acc: 0.171875\n",
      "epoch: 12  loss: 2.086735  acc: 0.140625\n",
      "epoch: 12  loss: 1.913126  acc: 0.234375\n",
      "epoch: 13  loss: 1.858434  acc: 0.312500\n",
      "epoch: 13  loss: 1.837262  acc: 0.218750\n",
      "epoch: 13  loss: 1.805617  acc: 0.265625\n",
      "epoch: 13  loss: 1.565035  acc: 0.343750\n",
      "epoch: 13  loss: 1.763960  acc: 0.234375\n",
      "epoch: 14  loss: 1.768775  acc: 0.281250\n",
      "epoch: 14  loss: 1.691239  acc: 0.296875\n",
      "epoch: 14  loss: 1.726952  acc: 0.281250\n",
      "epoch: 14  loss: 2.077000  acc: 0.156250\n",
      "epoch: 14  loss: 1.577620  acc: 0.375000\n",
      "epoch: 15  loss: 1.846084  acc: 0.312500\n",
      "epoch: 15  loss: 1.569550  acc: 0.343750\n",
      "epoch: 15  loss: 1.957229  acc: 0.218750\n",
      "epoch: 15  loss: 1.727154  acc: 0.296875\n",
      "epoch: 15  loss: 1.870849  acc: 0.234375\n",
      "epoch: 16  loss: 1.656250  acc: 0.375000\n",
      "epoch: 16  loss: 2.287036  acc: 0.312500\n",
      "epoch: 16  loss: 1.791948  acc: 0.296875\n",
      "epoch: 16  loss: 1.767007  acc: 0.234375\n",
      "epoch: 16  loss: 1.996172  acc: 0.390625\n",
      "epoch: 17  loss: 1.511146  acc: 0.375000\n",
      "epoch: 17  loss: 1.655085  acc: 0.296875\n",
      "epoch: 17  loss: 1.672304  acc: 0.343750\n",
      "epoch: 17  loss: 1.600589  acc: 0.343750\n",
      "epoch: 17  loss: 1.909505  acc: 0.343750\n",
      "epoch: 18  loss: 1.643580  acc: 0.296875\n",
      "epoch: 18  loss: 1.386443  acc: 0.437500\n",
      "epoch: 18  loss: 1.708976  acc: 0.421875\n",
      "epoch: 18  loss: 1.822535  acc: 0.390625\n",
      "epoch: 18  loss: 1.725301  acc: 0.421875\n",
      "epoch: 19  loss: 1.768121  acc: 0.406250\n",
      "epoch: 19  loss: 1.356799  acc: 0.484375\n",
      "epoch: 19  loss: 1.654394  acc: 0.343750\n",
      "epoch: 19  loss: 1.223907  acc: 0.484375\n",
      "epoch: 19  loss: 1.942776  acc: 0.390625\n",
      "epoch: 20  loss: 2.141697  acc: 0.328125\n",
      "epoch: 20  loss: 1.193382  acc: 0.515625\n",
      "epoch: 20  loss: 1.441708  acc: 0.484375\n",
      "epoch: 20  loss: 1.588557  acc: 0.359375\n",
      "epoch: 20  loss: 1.470511  acc: 0.390625\n",
      "epoch: 21  loss: 1.599399  acc: 0.453125\n",
      "epoch: 21  loss: 1.858019  acc: 0.406250\n",
      "epoch: 21  loss: 1.432159  acc: 0.437500\n",
      "epoch: 21  loss: 1.203432  acc: 0.515625\n",
      "epoch: 21  loss: 1.390600  acc: 0.421875\n",
      "epoch: 22  loss: 2.366311  acc: 0.343750\n",
      "epoch: 22  loss: 1.595368  acc: 0.359375\n",
      "epoch: 22  loss: 1.023911  acc: 0.562500\n",
      "epoch: 22  loss: 1.317163  acc: 0.453125\n",
      "epoch: 22  loss: 1.279964  acc: 0.453125\n",
      "epoch: 23  loss: 1.303110  acc: 0.437500\n",
      "epoch: 23  loss: 1.453843  acc: 0.500000\n",
      "epoch: 23  loss: 1.675985  acc: 0.515625\n",
      "epoch: 23  loss: 1.388537  acc: 0.453125\n",
      "epoch: 23  loss: 0.986572  acc: 0.593750\n",
      "epoch: 24  loss: 1.271364  acc: 0.453125\n",
      "epoch: 24  loss: 1.089091  acc: 0.546875\n",
      "epoch: 24  loss: 1.453928  acc: 0.375000\n",
      "epoch: 24  loss: 1.281384  acc: 0.468750\n",
      "epoch: 24  loss: 1.116367  acc: 0.531250\n",
      "epoch: 25  loss: 1.304595  acc: 0.453125\n",
      "epoch: 25  loss: 1.406378  acc: 0.531250\n",
      "epoch: 25  loss: 1.170952  acc: 0.500000\n",
      "epoch: 25  loss: 1.136840  acc: 0.546875\n",
      "epoch: 25  loss: 0.972898  acc: 0.656250\n",
      "epoch: 26  loss: 1.153510  acc: 0.531250\n",
      "epoch: 26  loss: 1.278924  acc: 0.500000\n",
      "epoch: 26  loss: 0.987393  acc: 0.578125\n",
      "epoch: 26  loss: 1.236272  acc: 0.484375\n",
      "epoch: 26  loss: 1.168228  acc: 0.531250\n",
      "epoch: 27  loss: 0.888116  acc: 0.640625\n",
      "epoch: 27  loss: 1.241707  acc: 0.609375\n",
      "epoch: 27  loss: 1.879571  acc: 0.531250\n",
      "epoch: 27  loss: 1.084928  acc: 0.546875\n",
      "epoch: 27  loss: 1.412807  acc: 0.593750\n",
      "epoch: 28  loss: 0.883454  acc: 0.625000\n",
      "epoch: 28  loss: 0.828205  acc: 0.671875\n",
      "epoch: 28  loss: 1.276142  acc: 0.609375\n",
      "epoch: 28  loss: 1.138265  acc: 0.656250\n",
      "epoch: 28  loss: 1.531142  acc: 0.734375\n",
      "epoch: 29  loss: 0.949396  acc: 0.593750\n",
      "epoch: 29  loss: 0.601559  acc: 0.750000\n",
      "epoch: 29  loss: 0.967480  acc: 0.625000\n",
      "epoch: 29  loss: 1.023829  acc: 0.562500\n",
      "epoch: 29  loss: 0.756474  acc: 0.843750\n",
      "epoch: 30  loss: 0.866520  acc: 0.625000\n",
      "epoch: 30  loss: 1.818239  acc: 0.656250\n",
      "epoch: 30  loss: 0.704102  acc: 0.718750\n",
      "epoch: 30  loss: 0.954786  acc: 0.671875\n",
      "epoch: 30  loss: 0.902011  acc: 0.625000\n",
      "epoch: 31  loss: 0.763561  acc: 0.687500\n",
      "epoch: 31  loss: 0.686118  acc: 0.718750\n",
      "epoch: 31  loss: 0.641071  acc: 0.734375\n",
      "epoch: 31  loss: 0.655371  acc: 0.734375\n",
      "epoch: 31  loss: 1.082333  acc: 0.703125\n"
     ]
    }
   ],
   "source": [
    "neurons = inputs + hidden + outputs\n",
    "A = torch.rand(neurons, neurons).requires_grad_(True)\n",
    "optimizer = torch.optim.Adam([A], lr=lr)\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "for epoch in range(0, epochs):\n",
    "    for i, (batch_X, batch_y) in enumerate(train_loader):\n",
    "        state = torch.zeros(batch_X.shape[0], neurons)\n",
    "        for t in range(0, ticks):\n",
    "            state = net(state, batch_X.view(-1, 28*28))\n",
    "            \n",
    "        outs = state[:,inputs+hidden:neurons]\n",
    "        loss = loss_fn(outs, batch_y)\n",
    "        \n",
    "        # Using pytorch backpropagation becouse our numpy implementaiton is inefficent\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        acc = (outs.max(1).indices == batch_y).float().mean()\n",
    "        \n",
    "        #mean_loss = 0.01*loss.item() + 0.99*mean_loss\n",
    "        #mean_acc = 0.01*acc.item() + 0.99*mean_acc\n",
    "        \n",
    "        if i % 200 == 0:\n",
    "            print(\"epoch: %d  loss: %f  acc: %f\" % (epoch, loss.item(), acc.item()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Testing Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "y_true = []\n",
    "y_pred = []\n",
    "\n",
    "for i, (batch_X, batch_y) in enumerate(test_loader):\n",
    "    state = torch.zeros(batch_X.shape[0], neurons)\n",
    "    for t in range(0, ticks):\n",
    "        state = net(state, batch_X.view(-1, 28*28))\n",
    "\n",
    "    outs = state[:,inputs+hidden:neurons]\n",
    "    preds = outs.max(1).indices\n",
    "    \n",
    "    y_true.extend(batch_y.detach().numpy().tolist())\n",
    "    y_pred.extend(preds.detach().numpy().tolist())\n",
    "\n",
    "print(\"Accuracy:\")\n",
    "print(accuracy_score(y_true, y_pred))\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(y_true, y_pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
