{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0. Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs=784 + 1 # bias input\n",
    "hidden=100\n",
    "outputs=10\n",
    "ticks = 3\n",
    "\n",
    "batch_size=64\n",
    "epochs = 100\n",
    "lr = 0.001"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(\n",
    "  torchvision.datasets.MNIST(\"/tmp/mnist_data\", train=True, download=True,\n",
    "                             transform=torchvision.transforms.Compose([\n",
    "                               torchvision.transforms.ToTensor(),\n",
    "                               torchvision.transforms.Normalize(\n",
    "                                 (0.1307,), (0.3081,))\n",
    "                             ])),\n",
    "  batch_size=batch_size, shuffle=True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "  torchvision.datasets.MNIST(\"/tmp/mnist_data\", train=False, download=True,\n",
    "                             transform=torchvision.transforms.Compose([\n",
    "                               torchvision.transforms.ToTensor(),\n",
    "                               torchvision.transforms.Normalize(\n",
    "                                 (0.1307,), (0.3081,))\n",
    "                             ])),\n",
    "  batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2 MNN forward definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def net(s, x):\n",
    "    s[:,0:inputs-1] = x\n",
    "    s[:,inputs] = 1\n",
    "    t = torch.matmul(s, A)\n",
    "    s = torch.relu(t)\n",
    "    return s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0  loss: 3372.194092  acc: 0.062500\n",
      "epoch: 0  loss: 26.474407  acc: 0.359375\n",
      "epoch: 0  loss: 6.621606  acc: 0.421875\n",
      "epoch: 0  loss: 8.578157  acc: 0.515625\n",
      "epoch: 0  loss: 5.717260  acc: 0.390625\n",
      "epoch: 1  loss: 27.072432  acc: 0.468750\n",
      "epoch: 1  loss: 8.286741  acc: 0.375000\n",
      "epoch: 1  loss: 2.454543  acc: 0.468750\n",
      "epoch: 1  loss: 6.169957  acc: 0.500000\n",
      "epoch: 1  loss: 1.970924  acc: 0.390625\n",
      "epoch: 2  loss: 7.954168  acc: 0.421875\n",
      "epoch: 2  loss: 1.510525  acc: 0.375000\n",
      "epoch: 2  loss: 30.964554  acc: 0.281250\n",
      "epoch: 2  loss: 6.005965  acc: 0.343750\n",
      "epoch: 2  loss: 6.202227  acc: 0.312500\n",
      "epoch: 3  loss: 25.473427  acc: 0.359375\n",
      "epoch: 3  loss: 1.553840  acc: 0.390625\n",
      "epoch: 3  loss: 3.897541  acc: 0.296875\n",
      "epoch: 3  loss: 10.986524  acc: 0.328125\n",
      "epoch: 3  loss: 1.484676  acc: 0.375000\n",
      "epoch: 4  loss: 4.381802  acc: 0.359375\n",
      "epoch: 4  loss: 12.233021  acc: 0.312500\n",
      "epoch: 4  loss: 8.849322  acc: 0.203125\n",
      "epoch: 4  loss: 1.799240  acc: 0.234375\n",
      "epoch: 4  loss: 19.747335  acc: 0.203125\n",
      "epoch: 5  loss: 7.137672  acc: 0.234375\n",
      "epoch: 5  loss: 2.619088  acc: 0.250000\n",
      "epoch: 5  loss: 1.872229  acc: 0.234375\n",
      "epoch: 5  loss: 1.994566  acc: 0.250000\n",
      "epoch: 5  loss: 3.376936  acc: 0.234375\n",
      "epoch: 6  loss: 5.223537  acc: 0.156250\n",
      "epoch: 6  loss: 2.053472  acc: 0.218750\n",
      "epoch: 6  loss: 1.986874  acc: 0.250000\n",
      "epoch: 6  loss: 2.014761  acc: 0.156250\n",
      "epoch: 6  loss: 1.834895  acc: 0.250000\n",
      "epoch: 7  loss: 2.181653  acc: 0.203125\n",
      "epoch: 7  loss: 1.944925  acc: 0.250000\n",
      "epoch: 7  loss: 2.050739  acc: 0.140625\n",
      "epoch: 7  loss: 2.145448  acc: 0.187500\n",
      "epoch: 7  loss: 2.051136  acc: 0.171875\n",
      "epoch: 8  loss: 2.009058  acc: 0.171875\n",
      "epoch: 8  loss: 2.120372  acc: 0.171875\n",
      "epoch: 8  loss: 2.034995  acc: 0.171875\n",
      "epoch: 8  loss: 2.194650  acc: 0.093750\n",
      "epoch: 8  loss: 2.384797  acc: 0.187500\n",
      "epoch: 9  loss: 1.949973  acc: 0.218750\n",
      "epoch: 9  loss: 1.947633  acc: 0.218750\n",
      "epoch: 9  loss: 2.029581  acc: 0.156250\n",
      "epoch: 9  loss: 2.202549  acc: 0.156250\n",
      "epoch: 9  loss: 2.086825  acc: 0.140625\n",
      "epoch: 10  loss: 2.027512  acc: 0.218750\n",
      "epoch: 10  loss: 2.041517  acc: 0.171875\n",
      "epoch: 10  loss: 2.015082  acc: 0.187500\n",
      "epoch: 10  loss: 1.978892  acc: 0.234375\n",
      "epoch: 10  loss: 1.952677  acc: 0.265625\n",
      "epoch: 11  loss: 1.985299  acc: 0.187500\n",
      "epoch: 11  loss: 2.077671  acc: 0.125000\n",
      "epoch: 11  loss: 2.697185  acc: 0.203125\n",
      "epoch: 11  loss: 1.985947  acc: 0.187500\n",
      "epoch: 11  loss: 2.685299  acc: 0.203125\n",
      "epoch: 12  loss: 3.820582  acc: 0.109375\n",
      "epoch: 12  loss: 2.082453  acc: 0.125000\n",
      "epoch: 12  loss: 2.054185  acc: 0.171875\n",
      "epoch: 12  loss: 2.086735  acc: 0.140625\n",
      "epoch: 12  loss: 1.913126  acc: 0.234375\n",
      "epoch: 13  loss: 1.858434  acc: 0.312500\n",
      "epoch: 13  loss: 1.837262  acc: 0.218750\n",
      "epoch: 13  loss: 1.805617  acc: 0.265625\n",
      "epoch: 13  loss: 1.565035  acc: 0.343750\n",
      "epoch: 13  loss: 1.763960  acc: 0.234375\n",
      "epoch: 14  loss: 1.768775  acc: 0.281250\n",
      "epoch: 14  loss: 1.691239  acc: 0.296875\n",
      "epoch: 14  loss: 1.726952  acc: 0.281250\n",
      "epoch: 14  loss: 2.077000  acc: 0.156250\n",
      "epoch: 14  loss: 1.577620  acc: 0.375000\n",
      "epoch: 15  loss: 1.846084  acc: 0.312500\n",
      "epoch: 15  loss: 1.569550  acc: 0.343750\n",
      "epoch: 15  loss: 1.957229  acc: 0.218750\n",
      "epoch: 15  loss: 1.727154  acc: 0.296875\n",
      "epoch: 15  loss: 1.870849  acc: 0.234375\n",
      "epoch: 16  loss: 1.656250  acc: 0.375000\n",
      "epoch: 16  loss: 2.287036  acc: 0.312500\n",
      "epoch: 16  loss: 1.791948  acc: 0.296875\n",
      "epoch: 16  loss: 1.767007  acc: 0.234375\n",
      "epoch: 16  loss: 1.996172  acc: 0.390625\n",
      "epoch: 17  loss: 1.511146  acc: 0.375000\n",
      "epoch: 17  loss: 1.655085  acc: 0.296875\n",
      "epoch: 17  loss: 1.672304  acc: 0.343750\n",
      "epoch: 17  loss: 1.600589  acc: 0.343750\n",
      "epoch: 17  loss: 1.909505  acc: 0.343750\n",
      "epoch: 18  loss: 1.643580  acc: 0.296875\n",
      "epoch: 18  loss: 1.386443  acc: 0.437500\n",
      "epoch: 18  loss: 1.708976  acc: 0.421875\n",
      "epoch: 18  loss: 1.822535  acc: 0.390625\n",
      "epoch: 18  loss: 1.725301  acc: 0.421875\n",
      "epoch: 19  loss: 1.768121  acc: 0.406250\n",
      "epoch: 19  loss: 1.356799  acc: 0.484375\n",
      "epoch: 19  loss: 1.654394  acc: 0.343750\n",
      "epoch: 19  loss: 1.223907  acc: 0.484375\n",
      "epoch: 19  loss: 1.942776  acc: 0.390625\n",
      "epoch: 20  loss: 2.141697  acc: 0.328125\n",
      "epoch: 20  loss: 1.193382  acc: 0.515625\n",
      "epoch: 20  loss: 1.441708  acc: 0.484375\n",
      "epoch: 20  loss: 1.588557  acc: 0.359375\n",
      "epoch: 20  loss: 1.470511  acc: 0.390625\n",
      "epoch: 21  loss: 1.599399  acc: 0.453125\n",
      "epoch: 21  loss: 1.858019  acc: 0.406250\n",
      "epoch: 21  loss: 1.432159  acc: 0.437500\n",
      "epoch: 21  loss: 1.203432  acc: 0.515625\n",
      "epoch: 21  loss: 1.390600  acc: 0.421875\n",
      "epoch: 22  loss: 2.366311  acc: 0.343750\n",
      "epoch: 22  loss: 1.595368  acc: 0.359375\n",
      "epoch: 22  loss: 1.023911  acc: 0.562500\n",
      "epoch: 22  loss: 1.317163  acc: 0.453125\n",
      "epoch: 22  loss: 1.279964  acc: 0.453125\n",
      "epoch: 23  loss: 1.303110  acc: 0.437500\n",
      "epoch: 23  loss: 1.453843  acc: 0.500000\n",
      "epoch: 23  loss: 1.675985  acc: 0.515625\n",
      "epoch: 23  loss: 1.388537  acc: 0.453125\n",
      "epoch: 23  loss: 0.986572  acc: 0.593750\n",
      "epoch: 24  loss: 1.271364  acc: 0.453125\n",
      "epoch: 24  loss: 1.089091  acc: 0.546875\n",
      "epoch: 24  loss: 1.453928  acc: 0.375000\n",
      "epoch: 24  loss: 1.281384  acc: 0.468750\n",
      "epoch: 24  loss: 1.116367  acc: 0.531250\n",
      "epoch: 25  loss: 1.304595  acc: 0.453125\n",
      "epoch: 25  loss: 1.406378  acc: 0.531250\n",
      "epoch: 25  loss: 1.170952  acc: 0.500000\n",
      "epoch: 25  loss: 1.136840  acc: 0.546875\n",
      "epoch: 25  loss: 0.972898  acc: 0.656250\n",
      "epoch: 26  loss: 1.153510  acc: 0.531250\n",
      "epoch: 26  loss: 1.278924  acc: 0.500000\n",
      "epoch: 26  loss: 0.987393  acc: 0.578125\n",
      "epoch: 26  loss: 1.236272  acc: 0.484375\n",
      "epoch: 26  loss: 1.168228  acc: 0.531250\n",
      "epoch: 27  loss: 0.888116  acc: 0.640625\n",
      "epoch: 27  loss: 1.241707  acc: 0.609375\n",
      "epoch: 27  loss: 1.879571  acc: 0.531250\n",
      "epoch: 27  loss: 1.084928  acc: 0.546875\n",
      "epoch: 27  loss: 1.412807  acc: 0.593750\n",
      "epoch: 28  loss: 0.883454  acc: 0.625000\n",
      "epoch: 28  loss: 0.828205  acc: 0.671875\n",
      "epoch: 28  loss: 1.276142  acc: 0.609375\n",
      "epoch: 28  loss: 1.138265  acc: 0.656250\n",
      "epoch: 28  loss: 1.531142  acc: 0.734375\n",
      "epoch: 29  loss: 0.949396  acc: 0.593750\n",
      "epoch: 29  loss: 0.601559  acc: 0.750000\n",
      "epoch: 29  loss: 0.967480  acc: 0.625000\n",
      "epoch: 29  loss: 1.023829  acc: 0.562500\n",
      "epoch: 29  loss: 0.756474  acc: 0.843750\n",
      "epoch: 30  loss: 0.866520  acc: 0.625000\n",
      "epoch: 30  loss: 1.818239  acc: 0.656250\n",
      "epoch: 30  loss: 0.704102  acc: 0.718750\n",
      "epoch: 30  loss: 0.954786  acc: 0.671875\n",
      "epoch: 30  loss: 0.902011  acc: 0.625000\n",
      "epoch: 31  loss: 0.763561  acc: 0.687500\n",
      "epoch: 31  loss: 0.686118  acc: 0.718750\n",
      "epoch: 31  loss: 0.641071  acc: 0.734375\n",
      "epoch: 31  loss: 0.655371  acc: 0.734375\n",
      "epoch: 31  loss: 1.082333  acc: 0.703125\n",
      "epoch: 32  loss: 0.867548  acc: 0.687500\n",
      "epoch: 32  loss: 0.756815  acc: 0.687500\n",
      "epoch: 32  loss: 0.851293  acc: 0.734375\n",
      "epoch: 32  loss: 0.685190  acc: 0.718750\n",
      "epoch: 32  loss: 0.798783  acc: 0.703125\n",
      "epoch: 33  loss: 1.235810  acc: 0.593750\n",
      "epoch: 33  loss: 0.647460  acc: 0.812500\n",
      "epoch: 33  loss: 1.072703  acc: 0.671875\n",
      "epoch: 33  loss: 0.771775  acc: 0.703125\n",
      "epoch: 33  loss: 0.567217  acc: 0.765625\n",
      "epoch: 34  loss: 0.768372  acc: 0.734375\n",
      "epoch: 34  loss: 0.771371  acc: 0.734375\n",
      "epoch: 34  loss: 0.623813  acc: 0.718750\n",
      "epoch: 34  loss: 1.019170  acc: 0.687500\n",
      "epoch: 34  loss: 0.448866  acc: 0.812500\n",
      "epoch: 35  loss: 0.719058  acc: 0.718750\n",
      "epoch: 35  loss: 0.735105  acc: 0.703125\n",
      "epoch: 35  loss: 0.521006  acc: 0.781250\n",
      "epoch: 35  loss: 0.814223  acc: 0.796875\n",
      "epoch: 35  loss: 0.719739  acc: 0.703125\n",
      "epoch: 36  loss: 1.166632  acc: 0.687500\n",
      "epoch: 36  loss: 0.725165  acc: 0.687500\n",
      "epoch: 36  loss: 0.776234  acc: 0.718750\n",
      "epoch: 36  loss: 1.017419  acc: 0.718750\n",
      "epoch: 36  loss: 0.412510  acc: 0.843750\n",
      "epoch: 37  loss: 0.601928  acc: 0.750000\n",
      "epoch: 37  loss: 0.560092  acc: 0.781250\n",
      "epoch: 37  loss: 0.694543  acc: 0.687500\n",
      "epoch: 37  loss: 0.645916  acc: 0.734375\n",
      "epoch: 37  loss: 0.807676  acc: 0.718750\n",
      "epoch: 38  loss: 0.612209  acc: 0.796875\n",
      "epoch: 38  loss: 0.418194  acc: 0.843750\n",
      "epoch: 38  loss: 0.738517  acc: 0.687500\n",
      "epoch: 38  loss: 0.427055  acc: 0.843750\n",
      "epoch: 38  loss: 0.883271  acc: 0.703125\n",
      "epoch: 39  loss: 0.726884  acc: 0.718750\n",
      "epoch: 39  loss: 0.750657  acc: 0.703125\n",
      "epoch: 39  loss: 0.926941  acc: 0.671875\n",
      "epoch: 39  loss: 0.973950  acc: 0.734375\n",
      "epoch: 39  loss: 0.461218  acc: 0.796875\n",
      "epoch: 40  loss: 0.540910  acc: 0.796875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 40  loss: 0.493053  acc: 0.796875\n",
      "epoch: 40  loss: 0.513560  acc: 0.796875\n",
      "epoch: 40  loss: 0.422060  acc: 0.812500\n",
      "epoch: 40  loss: 0.373325  acc: 0.828125\n",
      "epoch: 41  loss: 0.712073  acc: 0.734375\n",
      "epoch: 41  loss: 0.823605  acc: 0.656250\n",
      "epoch: 41  loss: 0.539759  acc: 0.765625\n",
      "epoch: 41  loss: 0.522847  acc: 0.843750\n",
      "epoch: 41  loss: 0.630685  acc: 0.718750\n",
      "epoch: 42  loss: 0.578489  acc: 0.781250\n",
      "epoch: 42  loss: 0.584081  acc: 0.750000\n",
      "epoch: 42  loss: 0.532507  acc: 0.765625\n",
      "epoch: 42  loss: 0.790715  acc: 0.671875\n",
      "epoch: 42  loss: 0.468103  acc: 0.812500\n",
      "epoch: 43  loss: 0.463063  acc: 0.812500\n",
      "epoch: 43  loss: 0.681275  acc: 0.734375\n",
      "epoch: 43  loss: 0.546149  acc: 0.781250\n",
      "epoch: 43  loss: 0.657347  acc: 0.750000\n",
      "epoch: 43  loss: 0.407549  acc: 0.828125\n",
      "epoch: 44  loss: 0.672337  acc: 0.781250\n",
      "epoch: 44  loss: 0.586630  acc: 0.812500\n",
      "epoch: 44  loss: 0.462358  acc: 0.828125\n",
      "epoch: 44  loss: 0.518188  acc: 0.781250\n",
      "epoch: 44  loss: 0.469771  acc: 0.843750\n",
      "epoch: 45  loss: 0.804015  acc: 0.718750\n",
      "epoch: 45  loss: 0.627097  acc: 0.750000\n",
      "epoch: 45  loss: 0.480961  acc: 0.828125\n",
      "epoch: 45  loss: 0.518798  acc: 0.781250\n",
      "epoch: 45  loss: 0.360521  acc: 0.843750\n",
      "epoch: 46  loss: 0.348721  acc: 0.843750\n",
      "epoch: 46  loss: 0.498257  acc: 0.796875\n",
      "epoch: 46  loss: 0.654651  acc: 0.765625\n",
      "epoch: 46  loss: 0.596343  acc: 0.812500\n",
      "epoch: 46  loss: 0.437188  acc: 0.828125\n",
      "epoch: 47  loss: 0.540560  acc: 0.765625\n",
      "epoch: 47  loss: 0.678808  acc: 0.796875\n",
      "epoch: 47  loss: 0.607600  acc: 0.781250\n",
      "epoch: 47  loss: 0.713168  acc: 0.750000\n",
      "epoch: 47  loss: 0.356692  acc: 0.859375\n",
      "epoch: 48  loss: 0.413019  acc: 0.828125\n",
      "epoch: 48  loss: 0.332313  acc: 0.859375\n",
      "epoch: 48  loss: 0.755400  acc: 0.734375\n",
      "epoch: 48  loss: 0.937429  acc: 0.656250\n",
      "epoch: 48  loss: 0.467500  acc: 0.843750\n",
      "epoch: 49  loss: 0.592124  acc: 0.765625\n",
      "epoch: 49  loss: 0.775607  acc: 0.734375\n",
      "epoch: 49  loss: 0.754947  acc: 0.703125\n",
      "epoch: 49  loss: 0.341899  acc: 0.875000\n",
      "epoch: 49  loss: 0.439550  acc: 0.843750\n",
      "epoch: 50  loss: 0.647730  acc: 0.734375\n",
      "epoch: 50  loss: 0.660909  acc: 0.765625\n",
      "epoch: 50  loss: 0.483996  acc: 0.843750\n",
      "epoch: 50  loss: 0.677023  acc: 0.859375\n",
      "epoch: 50  loss: 0.652705  acc: 0.734375\n",
      "epoch: 51  loss: 0.845458  acc: 0.781250\n",
      "epoch: 51  loss: 0.637343  acc: 0.734375\n",
      "epoch: 51  loss: 0.174478  acc: 0.937500\n",
      "epoch: 51  loss: 0.417601  acc: 0.906250\n",
      "epoch: 51  loss: 0.262667  acc: 0.875000\n",
      "epoch: 52  loss: 0.325102  acc: 0.921875\n",
      "epoch: 52  loss: 0.380748  acc: 0.890625\n",
      "epoch: 52  loss: 0.192980  acc: 0.937500\n",
      "epoch: 52  loss: 0.330363  acc: 0.875000\n",
      "epoch: 52  loss: 0.177056  acc: 0.937500\n",
      "epoch: 53  loss: 0.347995  acc: 0.859375\n",
      "epoch: 53  loss: 0.231040  acc: 0.953125\n",
      "epoch: 53  loss: 0.293067  acc: 0.906250\n",
      "epoch: 53  loss: 0.438933  acc: 0.875000\n",
      "epoch: 53  loss: 0.151137  acc: 0.953125\n",
      "epoch: 54  loss: 0.320934  acc: 0.875000\n",
      "epoch: 54  loss: 0.170624  acc: 0.953125\n",
      "epoch: 54  loss: 0.258607  acc: 0.906250\n",
      "epoch: 54  loss: 0.100139  acc: 0.968750\n",
      "epoch: 54  loss: 0.148169  acc: 0.953125\n",
      "epoch: 55  loss: 0.116303  acc: 0.921875\n",
      "epoch: 55  loss: 0.237188  acc: 0.906250\n",
      "epoch: 55  loss: 0.247302  acc: 0.906250\n",
      "epoch: 55  loss: 0.120611  acc: 0.968750\n",
      "epoch: 55  loss: 0.211775  acc: 0.937500\n",
      "epoch: 56  loss: 0.277677  acc: 0.890625\n",
      "epoch: 56  loss: 0.245356  acc: 0.906250\n",
      "epoch: 56  loss: 0.243569  acc: 0.921875\n",
      "epoch: 56  loss: 0.129547  acc: 0.984375\n",
      "epoch: 56  loss: 0.232268  acc: 0.890625\n",
      "epoch: 57  loss: 0.348903  acc: 0.859375\n",
      "epoch: 57  loss: 0.219445  acc: 0.953125\n",
      "epoch: 57  loss: 0.278201  acc: 0.906250\n",
      "epoch: 57  loss: 0.328390  acc: 0.890625\n",
      "epoch: 57  loss: 0.105738  acc: 0.968750\n",
      "epoch: 58  loss: 0.150196  acc: 0.937500\n",
      "epoch: 58  loss: 0.197310  acc: 0.953125\n",
      "epoch: 58  loss: 0.166587  acc: 0.937500\n",
      "epoch: 58  loss: 0.180123  acc: 0.953125\n",
      "epoch: 58  loss: 0.926147  acc: 0.843750\n",
      "epoch: 59  loss: 0.183442  acc: 0.937500\n",
      "epoch: 59  loss: 0.202698  acc: 0.890625\n",
      "epoch: 59  loss: 0.150979  acc: 0.937500\n",
      "epoch: 59  loss: 0.247010  acc: 0.937500\n",
      "epoch: 59  loss: 0.118925  acc: 0.953125\n",
      "epoch: 60  loss: 0.105287  acc: 0.953125\n",
      "epoch: 60  loss: 0.116402  acc: 0.984375\n",
      "epoch: 60  loss: 0.209638  acc: 0.906250\n",
      "epoch: 60  loss: 0.164207  acc: 0.937500\n",
      "epoch: 60  loss: 0.155243  acc: 0.937500\n",
      "epoch: 61  loss: 0.339846  acc: 0.859375\n",
      "epoch: 61  loss: 0.223412  acc: 0.937500\n",
      "epoch: 61  loss: 0.268571  acc: 0.906250\n",
      "epoch: 61  loss: 0.066220  acc: 0.984375\n",
      "epoch: 61  loss: 0.096752  acc: 0.968750\n",
      "epoch: 62  loss: 0.298458  acc: 0.921875\n",
      "epoch: 62  loss: 0.074948  acc: 0.984375\n",
      "epoch: 62  loss: 0.220257  acc: 0.937500\n",
      "epoch: 62  loss: 0.310150  acc: 0.875000\n",
      "epoch: 62  loss: 0.324065  acc: 0.953125\n",
      "epoch: 63  loss: 0.068908  acc: 0.968750\n",
      "epoch: 63  loss: 0.214526  acc: 0.921875\n",
      "epoch: 63  loss: 0.165079  acc: 0.968750\n",
      "epoch: 63  loss: 0.248128  acc: 0.921875\n",
      "epoch: 63  loss: 0.124292  acc: 0.953125\n",
      "epoch: 64  loss: 0.355648  acc: 0.906250\n",
      "epoch: 64  loss: 0.203225  acc: 0.937500\n",
      "epoch: 64  loss: 0.187632  acc: 0.937500\n",
      "epoch: 64  loss: 0.062257  acc: 0.984375\n",
      "epoch: 64  loss: 0.126909  acc: 0.953125\n",
      "epoch: 65  loss: 0.072054  acc: 0.984375\n",
      "epoch: 65  loss: 0.189884  acc: 0.937500\n",
      "epoch: 65  loss: 0.251527  acc: 0.921875\n",
      "epoch: 65  loss: 0.132535  acc: 0.953125\n",
      "epoch: 65  loss: 0.591059  acc: 0.937500\n",
      "epoch: 66  loss: 0.131484  acc: 0.953125\n",
      "epoch: 66  loss: 0.183649  acc: 0.921875\n",
      "epoch: 66  loss: 0.212389  acc: 0.921875\n",
      "epoch: 66  loss: 0.102693  acc: 0.968750\n",
      "epoch: 66  loss: 0.172012  acc: 0.937500\n",
      "epoch: 67  loss: 0.062328  acc: 0.984375\n",
      "epoch: 67  loss: 0.158684  acc: 0.937500\n",
      "epoch: 67  loss: 0.085392  acc: 0.968750\n",
      "epoch: 67  loss: 0.296337  acc: 0.921875\n",
      "epoch: 67  loss: 0.230551  acc: 0.937500\n",
      "epoch: 68  loss: 0.194993  acc: 0.937500\n",
      "epoch: 68  loss: 0.120781  acc: 0.953125\n",
      "epoch: 68  loss: 0.261892  acc: 0.937500\n",
      "epoch: 68  loss: 0.553260  acc: 0.968750\n",
      "epoch: 68  loss: 0.106059  acc: 0.953125\n",
      "epoch: 69  loss: 0.166102  acc: 0.953125\n",
      "epoch: 69  loss: 0.109043  acc: 0.953125\n",
      "epoch: 69  loss: 0.146017  acc: 0.953125\n",
      "epoch: 69  loss: 0.119812  acc: 0.953125\n",
      "epoch: 69  loss: 0.219835  acc: 0.921875\n",
      "epoch: 70  loss: 0.070180  acc: 0.968750\n",
      "epoch: 70  loss: 0.169533  acc: 0.968750\n",
      "epoch: 70  loss: 0.283389  acc: 0.921875\n",
      "epoch: 70  loss: 0.398048  acc: 0.890625\n",
      "epoch: 70  loss: 0.311286  acc: 0.906250\n",
      "epoch: 71  loss: 0.323759  acc: 0.937500\n",
      "epoch: 71  loss: 0.172482  acc: 0.937500\n",
      "epoch: 71  loss: 0.140528  acc: 0.953125\n",
      "epoch: 71  loss: 0.155644  acc: 0.937500\n",
      "epoch: 71  loss: 0.241272  acc: 0.921875\n",
      "epoch: 72  loss: 0.218284  acc: 0.921875\n",
      "epoch: 72  loss: 0.053393  acc: 0.984375\n",
      "epoch: 72  loss: 0.114740  acc: 0.984375\n",
      "epoch: 72  loss: 0.210855  acc: 0.953125\n",
      "epoch: 72  loss: 0.324569  acc: 0.906250\n",
      "epoch: 73  loss: 0.085038  acc: 0.968750\n",
      "epoch: 73  loss: 0.376752  acc: 0.875000\n",
      "epoch: 73  loss: 0.028672  acc: 1.000000\n",
      "epoch: 73  loss: 0.240775  acc: 0.875000\n",
      "epoch: 73  loss: 0.107863  acc: 0.968750\n",
      "epoch: 74  loss: 0.069676  acc: 0.968750\n",
      "epoch: 74  loss: 0.106619  acc: 0.937500\n",
      "epoch: 74  loss: 0.167125  acc: 0.937500\n",
      "epoch: 74  loss: 0.338464  acc: 0.859375\n",
      "epoch: 74  loss: 0.068610  acc: 0.968750\n",
      "epoch: 75  loss: 0.152153  acc: 0.937500\n",
      "epoch: 75  loss: 0.135959  acc: 0.937500\n",
      "epoch: 75  loss: 0.109087  acc: 0.953125\n",
      "epoch: 75  loss: 0.155690  acc: 0.937500\n",
      "epoch: 75  loss: 0.669376  acc: 0.843750\n",
      "epoch: 76  loss: 0.048453  acc: 0.984375\n",
      "epoch: 76  loss: 0.080358  acc: 0.968750\n",
      "epoch: 76  loss: 0.261168  acc: 0.953125\n",
      "epoch: 76  loss: 0.087265  acc: 0.984375\n",
      "epoch: 76  loss: 0.067641  acc: 0.968750\n",
      "epoch: 77  loss: 0.194224  acc: 0.953125\n",
      "epoch: 77  loss: 0.297424  acc: 0.921875\n",
      "epoch: 77  loss: 0.223918  acc: 0.921875\n",
      "epoch: 77  loss: 0.045079  acc: 0.984375\n",
      "epoch: 77  loss: 0.224542  acc: 0.937500\n",
      "epoch: 78  loss: 0.240781  acc: 0.953125\n",
      "epoch: 78  loss: 0.243452  acc: 0.953125\n",
      "epoch: 78  loss: 0.188310  acc: 0.921875\n",
      "epoch: 78  loss: 0.219799  acc: 0.937500\n",
      "epoch: 78  loss: 0.175078  acc: 0.937500\n",
      "epoch: 79  loss: 0.247274  acc: 0.906250\n",
      "epoch: 79  loss: 0.097285  acc: 0.984375\n",
      "epoch: 79  loss: 0.160381  acc: 0.937500\n",
      "epoch: 79  loss: 0.125243  acc: 0.937500\n",
      "epoch: 79  loss: 0.388885  acc: 0.890625\n",
      "epoch: 80  loss: 0.169658  acc: 0.937500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 80  loss: 0.094495  acc: 0.953125\n",
      "epoch: 80  loss: 0.130607  acc: 0.968750\n",
      "epoch: 80  loss: 0.174940  acc: 0.953125\n",
      "epoch: 80  loss: 0.077542  acc: 0.968750\n",
      "epoch: 81  loss: 0.128128  acc: 0.968750\n",
      "epoch: 81  loss: 0.129849  acc: 0.953125\n",
      "epoch: 81  loss: 0.142679  acc: 0.937500\n",
      "epoch: 81  loss: 0.196485  acc: 0.953125\n",
      "epoch: 81  loss: 0.110845  acc: 0.953125\n",
      "epoch: 82  loss: 0.060586  acc: 0.968750\n",
      "epoch: 82  loss: 0.079515  acc: 0.968750\n",
      "epoch: 82  loss: 0.103322  acc: 0.968750\n",
      "epoch: 82  loss: 0.130299  acc: 0.937500\n",
      "epoch: 82  loss: 0.176548  acc: 0.953125\n",
      "epoch: 83  loss: 0.080915  acc: 0.968750\n",
      "epoch: 83  loss: 0.175514  acc: 0.937500\n",
      "epoch: 83  loss: 0.170380  acc: 0.953125\n",
      "epoch: 83  loss: 0.178971  acc: 0.968750\n",
      "epoch: 83  loss: 0.094930  acc: 0.984375\n",
      "epoch: 84  loss: 0.004027  acc: 1.000000\n",
      "epoch: 84  loss: 0.045177  acc: 0.984375\n",
      "epoch: 84  loss: 0.041374  acc: 0.984375\n",
      "epoch: 84  loss: 0.182671  acc: 0.937500\n",
      "epoch: 84  loss: 0.004714  acc: 1.000000\n",
      "epoch: 85  loss: 0.135988  acc: 0.968750\n",
      "epoch: 85  loss: 0.040615  acc: 1.000000\n",
      "epoch: 85  loss: 0.135317  acc: 0.953125\n",
      "epoch: 85  loss: 0.206048  acc: 0.937500\n",
      "epoch: 85  loss: 0.045176  acc: 0.984375\n",
      "epoch: 86  loss: 0.097857  acc: 0.968750\n",
      "epoch: 86  loss: 0.060380  acc: 0.984375\n",
      "epoch: 86  loss: 0.145265  acc: 0.968750\n",
      "epoch: 86  loss: 0.118710  acc: 0.984375\n",
      "epoch: 86  loss: 0.110932  acc: 0.937500\n",
      "epoch: 87  loss: 0.361997  acc: 0.890625\n",
      "epoch: 87  loss: 0.001648  acc: 1.000000\n",
      "epoch: 87  loss: 0.124899  acc: 0.953125\n",
      "epoch: 87  loss: 0.051171  acc: 0.984375\n",
      "epoch: 87  loss: 0.175968  acc: 0.953125\n",
      "epoch: 88  loss: 0.166211  acc: 0.968750\n",
      "epoch: 88  loss: 0.054365  acc: 0.984375\n",
      "epoch: 88  loss: 0.130537  acc: 0.953125\n",
      "epoch: 88  loss: 0.160183  acc: 0.953125\n",
      "epoch: 88  loss: 0.090606  acc: 0.953125\n",
      "epoch: 89  loss: 0.153049  acc: 0.953125\n",
      "epoch: 89  loss: 0.050924  acc: 0.984375\n",
      "epoch: 89  loss: 0.288206  acc: 0.937500\n",
      "epoch: 89  loss: 0.184002  acc: 0.937500\n",
      "epoch: 89  loss: 0.138397  acc: 0.953125\n",
      "epoch: 90  loss: 0.177331  acc: 0.953125\n",
      "epoch: 90  loss: 0.157784  acc: 0.953125\n",
      "epoch: 90  loss: 0.140872  acc: 0.937500\n",
      "epoch: 90  loss: 0.095283  acc: 0.984375\n",
      "epoch: 90  loss: 0.162232  acc: 0.937500\n",
      "epoch: 91  loss: 0.094064  acc: 0.968750\n",
      "epoch: 91  loss: 0.064965  acc: 0.984375\n",
      "epoch: 91  loss: 0.231647  acc: 0.921875\n",
      "epoch: 91  loss: 0.191597  acc: 0.968750\n",
      "epoch: 91  loss: 0.124702  acc: 0.968750\n",
      "epoch: 92  loss: 0.003827  acc: 1.000000\n",
      "epoch: 92  loss: 0.193447  acc: 0.921875\n",
      "epoch: 92  loss: 0.024188  acc: 1.000000\n",
      "epoch: 92  loss: 0.012875  acc: 1.000000\n",
      "epoch: 92  loss: 0.075227  acc: 0.984375\n",
      "epoch: 93  loss: 0.183152  acc: 0.921875\n",
      "epoch: 93  loss: 0.094988  acc: 0.984375\n",
      "epoch: 93  loss: 0.171268  acc: 0.921875\n",
      "epoch: 93  loss: 0.051944  acc: 0.984375\n",
      "epoch: 93  loss: 0.083023  acc: 0.968750\n",
      "epoch: 94  loss: 0.152259  acc: 0.937500\n",
      "epoch: 94  loss: 0.278396  acc: 0.953125\n",
      "epoch: 94  loss: 0.091657  acc: 0.953125\n",
      "epoch: 94  loss: 0.188539  acc: 0.953125\n",
      "epoch: 94  loss: 0.056283  acc: 0.968750\n",
      "epoch: 95  loss: 0.124289  acc: 0.953125\n",
      "epoch: 95  loss: 0.145950  acc: 0.953125\n",
      "epoch: 95  loss: 0.041647  acc: 0.984375\n",
      "epoch: 95  loss: 0.132980  acc: 0.953125\n",
      "epoch: 95  loss: 0.088139  acc: 1.000000\n",
      "epoch: 96  loss: 0.125051  acc: 0.968750\n",
      "epoch: 96  loss: 0.075901  acc: 0.968750\n",
      "epoch: 96  loss: 0.102301  acc: 0.953125\n",
      "epoch: 96  loss: 0.187267  acc: 0.953125\n",
      "epoch: 96  loss: 0.120078  acc: 0.953125\n",
      "epoch: 97  loss: 0.064054  acc: 0.968750\n",
      "epoch: 97  loss: 0.105236  acc: 0.953125\n",
      "epoch: 97  loss: 0.052501  acc: 0.984375\n",
      "epoch: 97  loss: 0.117125  acc: 0.968750\n",
      "epoch: 97  loss: 0.014766  acc: 1.000000\n",
      "epoch: 98  loss: 0.117256  acc: 0.937500\n",
      "epoch: 98  loss: 0.117078  acc: 0.968750\n",
      "epoch: 98  loss: 0.183362  acc: 0.921875\n",
      "epoch: 98  loss: 0.145026  acc: 0.953125\n",
      "epoch: 98  loss: 0.272121  acc: 0.921875\n",
      "epoch: 99  loss: 0.280214  acc: 0.968750\n",
      "epoch: 99  loss: 0.122535  acc: 0.968750\n",
      "epoch: 99  loss: 0.192214  acc: 0.890625\n",
      "epoch: 99  loss: 0.265095  acc: 0.921875\n",
      "epoch: 99  loss: 0.096784  acc: 0.953125\n"
     ]
    }
   ],
   "source": [
    "neurons = inputs + hidden + outputs\n",
    "A = torch.rand(neurons, neurons).requires_grad_(True)\n",
    "optimizer = torch.optim.Adam([A], lr=lr)\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "for epoch in range(0, epochs):\n",
    "    for i, (batch_X, batch_y) in enumerate(train_loader):\n",
    "        state = torch.zeros(batch_X.shape[0], neurons)\n",
    "        for t in range(0, ticks):\n",
    "            state = net(state, batch_X.view(-1, 28*28))\n",
    "            \n",
    "        outs = state[:,inputs+hidden:neurons]\n",
    "        loss = loss_fn(outs, batch_y)\n",
    "        \n",
    "        # Using pytorch backpropagation becouse our numpy implementaiton is inefficent\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        acc = (outs.max(1).indices == batch_y).float().mean()\n",
    "        \n",
    "        #mean_loss = 0.01*loss.item() + 0.99*mean_loss\n",
    "        #mean_acc = 0.01*acc.item() + 0.99*mean_acc\n",
    "        \n",
    "        if i % 200 == 0:\n",
    "            print(\"epoch: %d  loss: %f  acc: %f\" % (epoch, loss.item(), acc.item()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Testing Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:\n",
      "0.9414\n",
      "Confusion Matrix:\n",
      "[[ 959    1    3    0    1    3    8    3    2    0]\n",
      " [   2 1116    3    1    0    1    3    4    5    0]\n",
      " [  32    2  965    7    3    1    7    6    8    1]\n",
      " [  19    0   12  940    2   11    2   10   12    2]\n",
      " [  18    0    5    0  921    1    8    5    5   19]\n",
      " [  32    3    1   21    5  796   10    3   15    6]\n",
      " [  23    1    4    0    5   10  910    1    4    0]\n",
      " [  12    8   11    4    4    0    0  977    3    9]\n",
      " [  20    4    3   10    6    4    6   11  904    6]\n",
      " [  16    7    1    6   14    3    0   24   12  926]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "y_true = []\n",
    "y_pred = []\n",
    "\n",
    "for i, (batch_X, batch_y) in enumerate(test_loader):\n",
    "    state = torch.zeros(batch_X.shape[0], neurons)\n",
    "    for t in range(0, ticks):\n",
    "        state = net(state, batch_X.view(-1, 28*28))\n",
    "\n",
    "    outs = state[:,inputs+hidden:neurons]\n",
    "    preds = outs.max(1).indices\n",
    "    \n",
    "    y_true.extend(batch_y.detach().numpy().tolist())\n",
    "    y_pred.extend(preds.detach().numpy().tolist())\n",
    "\n",
    "print(\"Accuracy:\")\n",
    "print(accuracy_score(y_true, y_pred))\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(y_true, y_pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
