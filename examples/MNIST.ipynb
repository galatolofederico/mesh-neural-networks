{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0. Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Download Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = fetch_openml('mnist_784', version=1, return_X_y=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Train/Test split and dataset normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "permutation = np.random.permutation(X.shape[0])\n",
    "X = X[permutation]\n",
    "y = y[permutation]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "X_train = X_train.reshape((X_train.shape[0], 28, 28))\n",
    "X_test = X_test.reshape((X_test.shape[0], 28, 28))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs=28 + 1 # bias input\n",
    "hidden=30\n",
    "outputs=10\n",
    "\n",
    "batch_size=10\n",
    "epochs = 1500\n",
    "lr = 0.001"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 MNN Functions (check Scikit 2D notebook for details)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x, derivative=False):\n",
    "    gt = (x > 0)\n",
    "    if derivative:\n",
    "        return 1 * gt\n",
    "    else:\n",
    "        return x * gt\n",
    "\n",
    "def softmax(x):\n",
    "    exps = np.exp(x - np.max(x))\n",
    "    return exps / np.expand_dims(np.sum(exps, axis=1), axis=1)\n",
    "\n",
    "\n",
    "def ce_loss(out, y, grad):\n",
    "    y = y.astype(int)\n",
    "    m = y.shape[0]\n",
    "    p = softmax(out)\n",
    "    log_likelihood = -np.log(p[range(m),y])\n",
    "    loss = np.mean(log_likelihood, axis=0)\n",
    "\n",
    "    de = p\n",
    "    de[range(m),y] -= 1\n",
    "    de = de/m\n",
    "    de = np.expand_dims(np.expand_dims(de, axis=2), axis=2)\n",
    "    grad = de*grad\n",
    "\n",
    "    return (loss, grad.sum(axis=1).sum(axis=0))\n",
    "\n",
    "def derivate(grad, t):\n",
    "    sn = np.zeros(shape=grad.shape)\n",
    "    sn[:,np.eye(neurons).astype(bool)] = state[:,np.newaxis]\n",
    "    sn = np.transpose(sn, (0,3,2,1))\n",
    "    return f(t, derivative=True)[:, np.newaxis, np.newaxis] * (np.matmul(grad, A) + sn)\n",
    "\n",
    "def net(state, grad, x, bs=batch_size, compute_grad=True):\n",
    "    # set inputs neurons state with input vector\n",
    "    state[:,0:inputs] = np.concatenate((x, np.ones((bs,1))), axis=1)\n",
    "    \n",
    "    # compute ti\n",
    "    t = np.matmul(state, A)\n",
    "    \n",
    "    # Forward propagate the gradients\n",
    "    grad = derivate(grad, t) if compute_grad else None\n",
    "    \n",
    "    # Compute new state\n",
    "    state = f(t)\n",
    "    \n",
    "    return state, grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Adam Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Adam:\n",
    "    def __init__(self, lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-8, decay=0., **kwargs):\n",
    "        \n",
    "        allowed_kwargs = {'clipnorm', 'clipvalue'}\n",
    "        for k in kwargs:\n",
    "            if k not in allowed_kwargs:\n",
    "                raise TypeError('Unexpected keyword argument '\n",
    "                                'passed to optimizer: ' + str(k))\n",
    "\n",
    "        self.__dict__.update(kwargs)\n",
    "        self.iterations = 0\n",
    "        self.lr = lr\n",
    "        self.beta_1 = beta_1\n",
    "        self.beta_2 = beta_2\n",
    "        self.decay = decay\n",
    "        self.epsilon = epsilon\n",
    "        self.initial_decay = decay\n",
    "\n",
    "    def step(self, params, grads):\n",
    "        original_shapes = [x.shape for x in params]\n",
    "        params = [x.flatten() for x in params]\n",
    "        grads = [x.flatten() for x in grads]\n",
    "        \n",
    "\n",
    "        lr = self.lr\n",
    "        if self.initial_decay > 0:\n",
    "            lr *= (1. / (1. + self.decay * self.iterations))\n",
    "\n",
    "        t = self.iterations + 1\n",
    "        lr_t = lr * (np.sqrt(1. - np.power(self.beta_2, t)) /\n",
    "                     (1. - np.power(self.beta_1, t)))\n",
    "\n",
    "        if not hasattr(self, 'ms'):\n",
    "            self.ms = [np.zeros(p.shape) for p in params]\n",
    "            self.vs = [np.zeros(p.shape) for p in params]\n",
    "    \n",
    "        ret = [None] * len(params)\n",
    "        for i, p, g, m, v in zip(range(len(params)), params, grads, self.ms, self.vs):\n",
    "            m_t = (self.beta_1 * m) + (1. - self.beta_1) * g\n",
    "            v_t = (self.beta_2 * v) + (1. - self.beta_2) * np.square(g)\n",
    "            p_t = p - lr_t * m_t / (np.sqrt(v_t) + self.epsilon)\n",
    "            self.ms[i] = m_t\n",
    "            self.vs[i] = v_t\n",
    "            ret[i] = p_t\n",
    "        \n",
    "        self.iterations += 1\n",
    "        \n",
    "        for i in range(len(ret)):\n",
    "            ret[i] = ret[i].reshape(original_shapes[i])\n",
    "        \n",
    "        return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neurons = inputs + hidden + outputs\n",
    "A = np.random.rand(neurons, neurons)\n",
    "optimizer = Adam(lr=lr)\n",
    "\n",
    "for epoch in range(0, epochs):\n",
    "    losses = 0\n",
    "    for i in range(0, len(X_train)//batch_size):\n",
    "        # Training batches\n",
    "        batch_X = X_train[i*batch_size:(i+1)*batch_size]\n",
    "        batch_Y = y_train[i*batch_size:(i+1)*batch_size]\n",
    "        \n",
    "        state = np.zeros(shape=(batch_size, neurons)) # Init state\n",
    "        grad = np.zeros(shape=(batch_size, neurons, neurons, neurons)) # Init gradients\n",
    "\n",
    "        # Update MNN in time\n",
    "        for t in range(0, 28):\n",
    "            state, grad = net(state, grad, batch_X[:, t, :])\n",
    "        \n",
    "        # Permute gradients for error function\n",
    "        grad = np.transpose(grad, (0,3,1,2))\n",
    "        \n",
    "        # Slice output values and gradients\n",
    "        outs = state[:,inputs+hidden:neurons]\n",
    "        outs_grad = grad[:,inputs+hidden:neurons]\n",
    "        \n",
    "        # Compute loss and error gradients\n",
    "        loss, err_grad = ce_loss(outs, batch_Y, outs_grad)\n",
    "        losses += loss\n",
    "\n",
    "        # Update weights\n",
    "        A = optimizer.step(A, err_grad)\n",
    "        \n",
    "        print(\"epoch: %d  loss: %f\" % (epoch, losses/(len(train_X)//(batch_size))))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
